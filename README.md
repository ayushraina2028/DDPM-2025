# DDPM (Denoising Diffusion Probabilistic Models) Implementation

A PyTorch implementation of Denoising Diffusion Probabilistic Models (DDPM) from scratch with support for multiple sampling methods including DDPM and DDIM sampling. This implementation features a custom U-Net architecture with self-attention and time embedding for high-quality image generation.

## ğŸŒŸ Features

- **Custom U-Net Architecture**: Implements a U-Net model with residual connections, self-attention, and sinusoidal time embeddings
- **Multiple Sampling Methods**: 
  - DDPM Original (Ho et al., 2020)
  - DDPM with Xâ‚€ prediction
  - DDIM Sampling (Song et al., 2020)
  - Accelerated DDIM Sampling
- **Flexible Noise Schedules**: Linear and cosine beta schedules
- **Training Pipeline**: Complete training loop with visualization and checkpoint saving
- **Pre-trained Models**: Includes trained models for MNIST and AFHQ datasets

## ğŸ“ Project Structure
Download the model weights for AFHQ from [here](https://drive.google.com/file/d/15moXiLwrs0lr2vl991Lj-IGvJauwlQU-/view?usp=sharing) and MNIST from [here](https://drive.google.com/file/d/1OeT1MvDtRV80xevrSJtyuXxP_zGBNfFh/view?usp=sharing) and keep those in their respective directories, so that the final project structure looks like as shown below:

```
ddpm_pipeline/
â”œâ”€â”€ models/                     # Model architectures
â”‚   â”œâ”€â”€ unet.py                # Main U-Net model
â”‚   â”œâ”€â”€ convBlockForUnet.py    # Convolutional blocks
â”‚   â”œâ”€â”€ selfAttention.py       # Self-attention mechanism
â”‚   â”œâ”€â”€ time_embedding.py      # Sinusoidal time embeddings
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ diffusion/                 # Diffusion algorithms
â”‚   â”œâ”€â”€ scheduler.py           # Noise scheduling and sampling
â”‚   â”œâ”€â”€ schedule_self.py       # Alternative scheduler implementation
â”‚   â””â”€â”€ trainer.py             # Training utilities
â”œâ”€â”€ diffusion_training_mnist/  # Pre-trained MNIST model
â”‚   â”œâ”€â”€ final_model.pt         # Trained model weights
â”‚   â”œâ”€â”€ loss_plot.png          # Training loss curve
â”‚   â””â”€â”€ progress.png           # Training progress visualization
â”œâ”€â”€ TrainedModel_AFHQ_64/     # Pre-trained AFHQ model (64x64)
â”‚   â””â”€â”€ diffusion_training_64/
â”‚       â””â”€â”€ final_model.pt     # Trained model weights
â”œâ”€â”€ Generations/               # Sample outputs
â”œâ”€â”€ GeneratedSamplesMNIST/     # MNIST sample outputs
â”œâ”€â”€ train.py                   # Training script
â”œâ”€â”€ sample.py                  # Sampling script
â””â”€â”€ requirements.txt           # Dependencies
```

## ğŸš€ Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/ayushraina2028/DDPM-2025.git
cd ddpm_pipeline
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Dependencies

- Python 3.8+
- PyTorch â‰¥ 1.9.0
- torchvision â‰¥ 0.10.0
- numpy â‰¥ 1.20.0
- matplotlib â‰¥ 3.4.0
- tqdm â‰¥ 4.61.0

## ğŸ¯ Usage

### Generate Samples (Using Pre-trained Models)

#### MNIST Generation
```bash
python sample.py \
    --model_path ./diffusion_training_mnist/final_model.pt \
    --num_samples 16 \
    --image_size 32 \
    --output_dir ./generated_mnist \
    --sampling_method ddpm_predicted_x0 \
    --inference_steps 1000 \
    --save_grid
```

#### AFHQ Generation (64x64)
```bash
python sample.py \
    --model_path ./TrainedModel_AFHQ_64/diffusion_training_64/final_model.pt \
    --num_samples 16 \
    --image_size 64 \
    --output_dir ./generated_afhq \
    --sampling_method ddim_sampling \
    --inference_steps 50 \
    --save_grid
```

### Training from Scratch

```bash
python train.py \
    --batch_size 64 \
    --epochs 100 \
    --lr 1e-4 \
    --output_dir ./my_training_output \
    --image_size 64 \
    --sampling_method ddpm_predicted_x0
```

### Sampling Methods

The implementation supports four different sampling methods:

1. **`ddpm_original`**: Original DDPM sampling as described in Ho et al. (2020)
2. **`ddpm_predicted_x0`**: DDPM with direct xâ‚€ prediction for faster convergence
3. **`ddim_sampling`**: DDIM sampling for deterministic generation with fewer steps
4. **`accelerated_ddim_sampling`**: Enhanced DDIM with improved acceleration

## ğŸ“Š Results

### MNIST Generation Results

| Method | Samples |
|--------|---------|
| DDPM Original | ![MNIST DDPM Original](./GeneratedSamplesMNIST/samples_grid_ddpm_original.png) |
| DDPM Xâ‚€ Prediction | ![MNIST DDPM X0](./GeneratedSamplesMNIST/samples_grid_ddpm_predicted_x0.png) |

### AFHQ Generation Results

| Method | Samples |
|--------|---------|
| DDPM Original | ![AFHQ DDPM Original](./Generations/samples_grid_ddpm_original.png) |
| DDPM Xâ‚€ Prediction | ![AFHQ DDPM X0](./Generations/samples_grid_ddpm_predicted_x0.png) |
| DDIM Sampling | ![AFHQ DDIM](./Generations/samples_grid_ddim_sampling.png) |
| Accelerated DDIM | ![AFHQ Accelerated DDIM](./Generations/samples_grid_accelerated_ddim_sampling.png) |

### Training Progress

| Dataset | Loss Curve | Sample Evolution |
|---------|------------|------------------|
| MNIST | ![MNIST Loss](./diffusion_training_mnist/loss_plot.png) | ![MNIST Progress](./diffusion_training_mnist/progress.png) |

## ğŸ—ï¸ Model Architecture

### U-Net Details
- **Input/Output Channels**: 3 (RGB) or 1 (grayscale)
- **Base Channels**: 64
- **Depth**: 3-4 levels with skip connections
- **Time Embedding**: 128-dimensional sinusoidal embeddings
- **Attention**: Self-attention at multiple resolutions
- **Activation**: SiLU (Swish) activation functions

### Noise Schedule
- **Linear Schedule**: Î²â‚€ = 0.0001, Î²â‚œ = 0.02
- **Cosine Schedule**: Improved schedule from Nichol & Dhariwal (2021)
- **Total Timesteps**: 1000 (customizable)

## ğŸ“š Implementation Details

### Key Components

1. **`SimpleUNetModel`**: Core denoising network with:
   - Encoder-decoder architecture with skip connections
   - Time-conditional layers
   - Self-attention mechanisms
   - Residual connections

2. **`Diffusion_Scheduler`**: Handles:
   - Noise scheduling (linear/cosine)
   - Forward diffusion process
   - Multiple sampling algorithms
   - Timestep scheduling for inference

3. **Training Pipeline**: Features:
   - Progressive sample visualization
   - Loss tracking and plotting
   - Model checkpointing
   - Support for different datasets

### Sampling Efficiency

| Method | Steps Required | Quality | Speed |
|--------|---------------|---------|-------|
| DDPM Original | 1000 | High | Slow |
| DDPM Xâ‚€ Prediction | 1000 | High | Slow |
| DDIM | 50-250 | High | Fast |
| Accelerated DDIM | 20-50 | High | Very Fast |

## ğŸ›ï¸ Configuration Options

### Training Parameters
- `--batch_size`: Training batch size (default: 64)
- `--epochs`: Number of training epochs (default: 50)
- `--lr`: Learning rate (default: 1e-4)
- `--image_size`: Input image resolution (default: 64)
- `--output_dir`: Directory for saving outputs

### Sampling Parameters
- `--num_samples`: Number of images to generate (default: 16)
- `--inference_steps`: Number of denoising steps (default: 1000)
- `--sampling_method`: Choice of sampling algorithm
- `--save_grid`: Save samples as a grid image

## ğŸ“„ Pre-trained Model Weights

### Available Models

1. **MNIST Model** (`./diffusion_training_mnist/final_model.pt`)
   - Resolution: 32Ã—32
   - Training: 50 epochs on MNIST dataset
   - Architecture: Depth=3, Base channels=64

2. **AFHQ Model** (`./TrainedModel_AFHQ_64/diffusion_training_64/final_model.pt`)
   - Resolution: 64Ã—64
   - Training: Animal faces (AFHQ dataset)
   - Architecture: Depth=4, Base channels=64

### Model Loading Example

```python
import torch
from models.unet import SimpleUNetModel

# Load pre-trained model
model = SimpleUNetModel(
    inChannels=3,
    outChannels=3,
    baseChannels=64,
    timeEmbeddingDimension=128,
    depth=3
)

# Load weights
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
state_dict = torch.load('path/to/model.pt', map_location=device, weights_only=True)
model.load_state_dict(state_dict)
model.to(device)
model.eval()
```

## ğŸ§ª Experimental Features

- **Multiple Beta Schedules**: Linear and cosine schedules
- **Accelerated Sampling**: Custom DDIM variants for faster generation
- **Flexible Architecture**: Configurable U-Net depth and channel sizes
- **Progress Visualization**: Real-time training progress monitoring

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

## ğŸ‘¥ Author

**Ayush Raina** - [ayushraina2028](https://github.com/ayushraina2028)

---

